\documentclass[12pt]{article}
\usepackage{bussproofs, array, amstext, amssymb} 
\usepackage{mathtools, extarrows}
\usepackage{latexsym}
\usepackage{syntax}
\usepackage{tabularx}
\usepackage{flowchart}
\usepackage{array,multirow}
\usepackage[section]{placeins}
\usepackage[bottom]{footmisc}
\usepackage{tablefootnote}
% \usepackage[nodayofweek,level]{datetime}
\usepackage[affil-it]{authblk}
\usepackage{hyperref}
\usepackage{algorithm,algpseudocode}
\usepackage{textcomp}
\usepackage{pgfplots}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{catchfilebetweentags}
\usepackage{agda}
\usepackage[verbose]{newunicodechar}
\usepackage{bigfoot}
\usepackage{makecell}

\input{unicode}

\pgfplotsset{compat=1.14}
\definecolor{VTV}{HTML}{019529}
\definecolor{OTV}{HTML}{0C06F7}

\algrenewcommand{\algorithmiccomment}[1]{\hskip3em$\rhd$ #1}
\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algdef{SE}[SWITCH]{Case}{EndCase}[1]{\textbf{case}\ #1\ \textbf{of}}{\algorithmicend\ \algorithmicswitch}%
\algtext*{Indent}
\algtext*{EndIndent}
\algtext*{EndCase}
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFunction}% Remove "end if" text

\renewcommand{\labelitemii}{$\star$}
\newcommand{\midtilde}{\raisebox{0.5ex}{\texttildelow}}
\newcommand{\at}[0]{\ @\ }
\newcommand{\An}[0]{\mathrm{An}}
\newcommand{\len}[0]{\mathrm{len}}
\newcommand{\Gnd}[0]{\mathit{gnd}}
\newcommand{\Prf}[0]{\mathit{prf}}
\newcommand{\Bch}[0]{\mathit{bch}}
\newcommand{\Bchs}[0]{\mathit{bchs}}
\newcommand{\Prob}[0]{\mathit{prob}}
\newcommand{\idf}[1]{\#_{#1}} 
\newcommand{\fresh}[1]{\mathit{fresh}_{\#}({#1})} 
\newcommand{\lfi}[0]{\mathrm{lfi}} 
\newcommand{\Dec}[1]{\xRightarrow{#1}} 
\newcommand{\bprf}[0]{\begin{prooftree}}
\newcommand{\eprf}[0]{\end{prooftree}}

\newcommand{\axm}[1]{\AxiomC{$#1$}}
\newcommand{\unr}[1]{\UnaryInfC{$#1$}}
\newcommand{\bnr}[1]{\BinaryInfC{$#1$}}
\newcommand{\tnr}[1]{\TrinaryInfC{$#1$}}
\newcommand{\rlb}[1]{\RightLabel{#1}}
\newcommand{\limp}[0]{\to}
\newcommand{\liff}[0]{\leftrightarrow}
\newcommand{\sq}[1]{\text{`#1'}} 

\newcommand{\size}[0]{\mathrm{size}}
\newcommand{\adm}[0]{\mathrm{adm}}

 

\title{A Formally Verified Checker for\\ First-Order Proofs}

\author{Seulkee Baek}
\date{}

\affil{Department of Philosophy, Carnegie Mellon University}
\affil{\textit{seulkeeb@andrew.cmu.edu}}

\begin{document}

\maketitle

\begin{abstract}

The Verified TESC Verifier (VTV) is a formally verified checker for the new 
Theory-Extensible Sequent Calculus (TESC) proof format for first-order ATPs. 
VTV accepts a TPTP problem and a TESC proof as input, and uses the latter to 
verify the unsatisfiability of the former. VTV is written in Agda, and the 
soundness of its proof-checking kernel is verified in respect to a first-order
semantics formalized in Agda. VTV shows robust performance in a comprehensive 
test using all eligible problems from the TPTP problem library, successfully 
verifying all but the largest 5 of 12296 proofs, with >97\% of the 
proofs verified in less than 1 second. 

\end{abstract}

\section{Introduction}

Modern automated reasoning tools are highly complex pieces of software, 
and it is generally no simple matter to establish their correctness.
Bugs have been discovered in them in the past \cite{reger2017testing,harrison2006towards}, 
and more are presumably hidden in systems used today. 
One popular strategy for coping with the possibility of errors 
in automated reasoning is the \textit{De Bruijn} criterion \cite{barendregt2005challenge}, 
which states that automated reasoning software should produce `proof objects' which can be 
independently verified by simple checkers that users can easily understand 
and trust. In addition to reducing the trust base of theorem provers, 
the De Bruijn criterion comes with the additional benefit that the 
small trusted core is often simple enough to be formally verified itself. 
Such thoroughgoing verification is far from universal, but there has been notable 
progress toward this goal in various subfields of automated reasoning, including 
interactive theorem provers, SAT solvers, and SMT solvers.

One area in which similar developments have been conspicuously absent is 
first-order automated theorem provers (ATPs), where the lack of a machine-checkable
proof format \cite{reger2017checkable} precluded the use of simple independent verifiers. 
The Theory-Extensible Sequent Calculus (TESC) is a new 
proof format for first-order ATPs designed to fill this gap. In particular,
the format's small set of fined-grained inference rules makes it relatively 
easy to implement and verify its proof checker.

This paper presents the Verified TESC Verifier (VTV), a proof checker for  
the TESC format \footnote{\url{https://github.com/skbaek/tesc}} written and verified 
in Agda \cite{bove2009brief}. The aim of the paper is twofold. Its immediate purpose is 
to demonstrate the reliability of TESC proofs by showing precisely what is established 
by their successful verification using VTV. Many of the implementation issues we shall 
discuss, however, are relevant to any formalization of first-order logic and not 
limited to either VTV or Agda. Therefore, the techniques VTV uses to solve them may be 
useful for other projects that reason about and perform computations with first-order 
logic data structures.

The rest of the paper is organized as follows:
Section \ref{sec:rel-works} gives a brief survey of similar works and how VTV relates to them.
Section \ref{sec:proof-calc} describes the syntax and inference rules of the TESC proof calculus.
Section \ref{sec:verifier} presents the main TESC verifier kernel, and
Section \ref{sec:soundness} gives a detailed specification of the verifier's soundness property.
Sections \ref{sec:proof-calc}, \ref{sec:verifier}, and \ref{sec:soundness} also include code 
excerpts and discuss how their respective contents are formalized in Agda.
Section \ref{sec:test-results} shows the  results of empirical tests measuring VTV's performance.
Section \ref{sec:conclusion} gives a summary and touches on potential future work.

\section{Related Works} \label{sec:rel-works}

SAT solving is arguably the most developed subfield of automated reasoning in terms of 
verified proof checkers. A non-exhaustive list of SAT proof formats with verified checkers
include RAT \cite{heule2013verifying}, RUP and IORUP \cite{heule2014bridging},
LRAT \cite{cruz2017lrat}, and GRIT \cite{cruz2017grit}. In the related field of SMT solving,
the SMTCoq project \cite{armand2011modular} also uses a proof checker implemented and
verified in the Coq proof assistant.

Despite the limitations imposed by G\"odel's second incompleteness theorem \cite{godel1931formal},
there has been interesting work toward verification of interactive theorem provers. 
All of HOL Light except the axiom of infinity has been proven consistent in HOL Light itself \cite{harrison2006towards}, 
which was further extended later to include definitional principles for new types 
and constants \cite{kumar2014hol}. There are also recent projects that go 
beyond the operational semantics of programming languages and verifies interactive theorem provers 
closer to the hardware, such as the Milawa theorem prover which runs on a Lisp runtime
verified against x86 machine code \cite{davis2015reflective}, and the Metamath Zero 
\cite{carneiro2019metamath} theorem prover which targets x84-64 instruction set architecture. 

VTV is designed to serve a role similar to these verified checkers for 
first-order ATPs and the TESC format. There has been several different 
approaches \cite{sutcliffe2006semantic, chihani2015proof} to verifying 
the output of first-order ATPs, but the only example I am aware of
which uses independent proof objects with a verified checker is the 
Ivy system \cite{mccune2000ivy}. The main difference between Ivy and 
VTV is that Ivy's proof objects only record resolution and paramodulation 
steps, so all input problems have to be normalized by a separate 
preprocessor written in ACL2. In contrast, the TESC format supports 
preprocessing steps like Skolemization and CNF normalization, and 
allows ATPs to work directly on input problems with their optimized 
preprocessing strategies.



\section{Proof Calculus} \label{sec:proof-calc} 

The syntax of the TESC calculus is as follows:
\begin{align*}
f ::= &\ \sigma\ |\ \idf{k}\\
t ::= &\ x_k\ |\ f(\vec{t})\\
\vec{t} ::= &\ \cdot\ |\ \vec{t}, t\\
\phi ::= &\ \top\ |\ \bot\ |\ f(\vec{t})\ |\ \lnot \phi\ |\ \phi \lor \chi\ |\ \phi \land \chi\ |\ \phi \to \chi\ |\ \phi \leftrightarrow \chi\ |\ \forall \phi\ |\ \exists \phi\\
\Gamma ::= &\ \cdot\ |\ \Gamma, \phi
\end{align*}
We let $f$ range over \textit{functors}, which are usually called `non-logical symbols' in
other presentations of first-order logic. The TESC calculus makes no distinction
between function and relation symbols, and relies on the context to determine whether 
a symbol applied to arguments is a term or an atomic formula. For brevity, we borrow
the umbrella term `functor' from the TPTP syntax and use it to refer to any non-logical symbol.

There are two types of functors: $\sigma$ ranges over \textit{plain functors}, 
which you can think of as the usual relation or function symbols. We assume that there 
is a suitable set of symbols $\Sigma$, and let $\sigma \in \Sigma$.
Symbols of the form $\idf{k}$ are \textit{indexed functors}, and the number $k$ is 
called the \textit{functor index} of $\idf{k}$. Indexed functors are used to reduce 
the cost of introducing fresh functors: if you keep track of the largest functor index
$k$ that occurs in the environment, you may safely use $\idf{k+1}$ as a fresh functor
without costly searches over a large number of terms and formulas. \footnote{Thanks to Marijn Heule for suggesting this idea.}

We use $t$ to range over terms, $\vec{t}$ over lists of terms, $\phi$ over formulas, 
and $\Gamma$ over sequents. Quantified formulas are written without variables thanks 
to the use of De Bruijn indices \cite{de1972lambda}; the number $k$ in variable $x_k$ is its De Bruijn index. 
As usual, parentheses may be inserted for scope disambiguation, and the empty list operator 
`$\cdot$' may be omitted when it appears as part of a complex expression. E.g., the sequent 
$\cdot, \phi, \psi$ and term $f(\cdot)$ may be abbreviated to $\phi, \psi$ and $f$.

Formalization of TESC syntax in Agda is mostly straightforward, but with one
small caveat. Consider the following definition of the type of terms:
\ExecuteMetaData[basic.tex]{term}
The constructor \AgdaInductiveConstructor{fun} builds a complex term out of a 
functor and a list of arguments. Since these arguments are behind a \AgdaFunction{List},
they are not immediate subterms of the complex term, and Agda cannot automatically 
prove termination for recursive calls that use them. For instance, 
\AgdaFunction{term-vars<?} \AgdaSymbol{:} \AgdaDatatype{Nat}
\AgdaSymbol{→} \AgdaDatatype{Term} \AgdaSymbol{→} \AgdaDatatype{Bool} is 
a function such that \AgdaFunction{term-vars<?} \AgdaBound{k} \AgdaBound{t} 
evaluates to \AgdaInductiveConstructor{true} iff all variables in \AgdaBound{t} 
have indices smaller than \AgdaBound{k}. It would be natural to define this
function as 
\ExecuteMetaData[other.tex]{tvlwrong}
where \AgdaBound{m} \AgdaPrimitive{<ᵇ} \AgdaBound{k} evaluates to 
\AgdaInductiveConstructor{true} iff \AgdaBound{m} is less than \AgdaBound{k}.
But Agda rejects this definition because it cannot prove that the recursive 
calls terminate. We can get around this problem by a mutual recursion 
between a pair of functions, one for terms and one for lists of terms: 
\ExecuteMetaData[verify.tex]{tvlright}
All other functions that recurse on terms are also defined using similar
mutual recursion.

\input{rules.tex}

The inference rules of the TESC calculus are shown in Table \ref{tab:inf-rules}.
The TESC calculus is a one-sided first-order sequent calculus, so having 
a valid TESC proof of a sequent $\Gamma$ shows that $\Gamma$ is collectively unsatisfiable.
The $A$,$B$,$C$,$D$, and $N$ rules are the \textit{analytic} rules. 
Analytic rules are similar to the usual one-sided sequent calculus rules, 
except that each analytic rule is overloaded to handle several connectives at once. 
For example, consider the formulas $\phi \land \psi$, $\lnot (\phi \lor \psi)$, 
$\lnot (\phi \limp \psi)$, and $\phi \liff \psi$. In usual sequent calculi, you would 
need a different rule for each of the connectives $\land$, $\lor$, $\limp$, and $\liff$ to 
break down these formulas. But all four formulas are ``essentially conjunctive'' 
in the sense that the latter three are equivalent to $\lnot \phi \land \lnot \psi$, 
$\phi \land \lnot \psi$, and $(\phi \limp \psi) \land (\psi \limp \phi)$.
So it is more convenient to handle all four of them with a single rule that analyzes 
a formula into its left and right conjuncts, which is the analytic $A$ rule.
Similarly, the $B$, $C$, $D$ rules are used to analyze essentially disjunctive, 
universal, existential formulas, and the $N$ rule performs double-negation elimination. 
For a complete list of formula analysis functions that show how each analytic 
rule breaks down formulas, see Appendix \ref{apdx:faf}. The analytic rules are 
a slightly modified adaptation of Smullyan's \textit{uniform notation} for analytic 
tableaux \cite{smullyan1995first}, which is where they get their names from. 

All rules are designed to preserve the invariant $\lfi(\Gamma) < \size(\Gamma)$ 
for every sequent $\Gamma$. We say that a sequent $\Gamma$ is \textit{good} if it satisfies 
this invariant. It is important that all sequents are good, because this is what ensures 
that the indexed functor $\idf{\size(\Gamma)}$ is fresh in respect to a sequent $\Gamma$.

Of the three remaining rules, $S$ is the usual cut rule, and $X$ is the axiom or init rule. 
The $T$ rule may be used to add \textit{admissable} formulas. A formula $\phi$ is admissable 
in respect to a target theory $T$ and sequent size $k$ if it satisfies the following condition:  
\begin{itemize}
  \item For any good sequent $\Gamma$ that is satisfiable modulo $T$ and $\size(\Gamma) = k$, the 
sequent $\Gamma, \phi$ is also satisfiable modulo $T$. 
\end{itemize}
More intuitively, the $T$ rule allows you add formulas that preserve satisfiability. 
Notice that the definition of admissability, and hence the definition of well-formed 
TESC proofs, depends on the implicit target theory. This is the `theory-extensible' part of TESC. 
The current version of VTV verifies basic TESC proofs that target the theory of equality, so it allows $T$ 
rules to introduce equality axioms, fresh relation symbol definitions, and choice axioms.
But VTV can be easily modified in a modular way to target other theories as well.

TESC proofs are formalized in Agda as an inductive type \AgdaFunction{Proof},
which has a separate constructor for each TESC inference rule: 
\ExecuteMetaData[verify.tex]{proof}
For example, the constructor \AgdaInductiveConstructor{rule-a} takes  
\AgdaFunction{Nat} and \AgdaFunction{Bool} arguments because this is the 
information necessary to specify an application of the $A$ rule. I.e.,  
\AgdaInductiveConstructor{rule-a} \AgdaBound{i} \AgdaBound{b} \AgdaBound{p} 
is a proof whose last inference rule adds the formula $A(b, \Gamma[i])$. 
Notice that sequents are completely absent from the definition of \AgdaFunction{Proof}.
This is a design choice made in favor of efficient space usage. Since TESC proofs are uniquely 
determined by their root sequents together with complete information of the inference rules used,
TESC files conserve space by omitting any components that can be constructed on the fly during 
verification, which includes all intermediate sequents and formulas introduced by analytic rules. 
\AgdaFunction{Proof} is designed to only record information included in TESC files, 
since terms of the type \AgdaFunction{Proof} are constructed by parsing input TESC files.



\section{The Verifier} \label{sec:verifier} 

The checker function \AgdaFunction{verify} for \AgdaFunction{Proof} performs two tasks: 
(1) it constructs the omitted intermediate sequents as it recurses down a \AgdaFunction{Proof}, and 
(2) it checks that the conditions are satisfied for each inference rule used. For instance, 
consider the definition of \AgdaFunction{verify} for the $C$ rule case, together with  
its type signature:
\ExecuteMetaData[verify.tex]{verify-c}
The terms (\AgdaFunction{nth} \AgdaBound{i} \AgdaBound{Γ}),  
(\AgdaFunction{analyze-c} \AgdaBound{t} (\AgdaFunction{nth} \AgdaBound{i} \AgdaBound{Γ})),  
and (\AgdaFunction{add} \AgdaBound{Γ} (\AgdaFunction{analyze-c} \AgdaBound{t} (\AgdaFunction{nth} \AgdaBound{i} \AgdaBound{Γ})))
each corresponds to $\Gamma[i]$, $C(t, \Gamma[i])$, and $\Gamma, C(t, \Gamma[i])$.
The conjunct \AgdaFunction{term-lfi<? (\AgdaInductiveConstructor{suc} (\AgdaFunction{size} \AgdaBound{Γ})) \AgdaBound{t}}
ensures the side condition $\lfi(t) \le \size(\Gamma)$ is satisfied, and the recursive call to 
\AgdaFunction{verify} checks that the subproof \AgdaBound{p} is a valid proof of
the sequent $\Gamma, C(t, \Gamma[i])$. The cases for other rules are also defined similarly.

The argument type \AgdaFunction{Sequent} for \AgdaFunction{verify} presents some 
interesting design choices. What kind of data structures should be used 
to encode sequents? The first version of VTV used lists, but lists immediately become 
a bottleneck with practically-sized problems due to their poor random access speeds. 
The default TESC verifier included in the TPTP-TSTP-TESC Processor (T3P, the main 
tool for generating and verifying TESC files) uses arrays, 
but arrays are hard to come by and even more difficult to reason about in dependently 
typed languages like Agda. Self-balancing trees like AVL or red-black trees come 
somewhere between lists and arrays in terms of convenience and performance, but it 
can still be tedious to prove basic facts about them if those proofs are not 
available in your language of choice, as is the case for Agda's standard library.

For VTV, we cut corners by taking advantage of the fact that (1) formulas 
are never deleted from sequents, and (2) new formulas are always added to the 
right end of sequents. (1) and (2) allow us to use a simple balancing algorithm.
In order to use the algorithm, we first define the type of trees in a way that 
allows each tree to efficiently keep track of its own size:
\ExecuteMetaData[basic.tex]{tree}
For any non-leaf tree \AgdaInductiveConstructor{fork} \AgdaBound{k} \AgdaBound{a} \AgdaBound{t} \AgdaBound{s},
the number \AgdaBound{k} is the size of \AgdaInductiveConstructor{fork} \AgdaBound{k} \AgdaBound{a} \AgdaBound{t} \AgdaBound{s}.
This property is not guaranteed to hold by construction, but it is easy to ensure that it always holds in practice. 
Since sizes of trees are stored as \AgdaFunction{Nat}, the function 
\AgdaFunction{size} \AgdaSymbol{:} \AgdaFunction{Tree} \AgdaSymbol{→} \AgdaFunction{Nat}
can always report the size of trees in constant time. New elements can be added to trees 
in a balanced way as follows: when adding an element to a non-leaf tree, 
compare its subtree sizes. If the right subtree is smaller, make a recursive call and add to the 
right subtree. If the sizes are equal, make a new tree that contains the current tree as its left subtree, 
an empty tree as its right subtree, and stores the new element in the root node.
The definition of the \AgdaFunction{add} function for trees implements this algorithm precisely:
\ExecuteMetaData[basic.tex]{add}
This addition algorithm does not keep the tree maximally balanced, but it provides 
reasonable performance and does away with complex ordering and balancing mechanisms, 
which makes reasoning about trees considerably easier. Given the type \AgdaFunction{Tree}, 
the type \AgdaFunction{Sequent} can be simply defined as \AgdaFunction{Tree Formula}.



\section{Soundness} \label{sec:soundness}

In order to prove the soundness of \AgdaFunction{verify}, we first need to formalize 
a first-order semantics that it can be sound in respect to. Most of the formalization 
is routine, but it also includes some oddities particular to VTV.

One awkward issue that recurs in formalization of first-order semantics is the 
handling of arities. Given that each functor has a unique arity,
what do you do with ill-formed terms and atomic formulas with the wrong 
number of arguments? You must either tweak the syntax definition to 
preclude such possibilities, or deal with ill-formed terms and formulas as 
edge cases, both of which can lead to bloated definitions and proofs. 

For VTV, we avoid this issue by assuming that every functor has infinite arities.
Or rather, for each functor $f$ with arity $k$, there are an infinite number 
of other functors that share the name $f$ and has arities $0, 1, ..., k-1, k+1, k+2, ...$ 
ad infinitum. With this assumption, the denotation of functors can be defined as 
\ExecuteMetaData[sound.tex]{relfun}
where \AgdaBound{D} is the type of the domain of discourse that the 
soundness proof is parametrized over.
A \AgdaFunction{Rels} (resp. \AgdaFunction{Funs}) can be thought of as a 
collection of an infinite number of relations (resp. functions), one for 
each arity. An interpretation is a pair of a relation assignment and a 
function assignment, which assign \AgdaFunction{Rels} and \AgdaFunction{Funs} 
to functors.
\ExecuteMetaData[sound.tex]{rafa}
Variable assignments assign denotations to \AgdaFunction{Nat}, since variables are 
identified by their Bruijn indices.
\ExecuteMetaData[sound.tex]{va}
For reasons we've discussed in Section \ref{sec:proof-calc}, term valuation 
requires a pair of mutually recursive functions in order to recurse on terms:
\ExecuteMetaData[sound.tex]{termval}
Formula valuation is mostly straightforward, but some care needs to be taken 
in the handling of variable assignments and quantified formulas. 
The function \AgdaInductiveConstructor{qtf} \AgdaSymbol{:} \AgdaFunction{Bool} \AgdaSymbol{→} \AgdaFunction{Formula} \AgdaSymbol{→} \AgdaFunction{Formula} 
is a constructor of \AgdaFunction{Formula} for quantified formulas, where 
\AgdaInductiveConstructor{qtf} \AgdaInductiveConstructor{false} and
\AgdaInductiveConstructor{qtf} \AgdaInductiveConstructor{true} encode the universal and existential quantifiers, respectively.
Given a relation assignment \AgdaBound{R},
function assignment   \AgdaBound{F}, and
variable assignment   \AgdaBound{V},
the values of formulas 
\AgdaInductiveConstructor{qtf} \AgdaInductiveConstructor{false} \AgdaBound{ϕ} and
\AgdaInductiveConstructor{qtf} \AgdaInductiveConstructor{true} \AgdaBound{ϕ} under
\AgdaBound{R}, \AgdaBound{F}, and \AgdaBound{V} are defined as
\ExecuteMetaData[sound.tex]{qtf-val}
The notations 
\AgdaSymbol{∀} \AgdaBound{x} \AgdaSymbol{→} \AgdaBound{A} and
\AgdaSymbol{∃} \AgdaSymbol{λ} \AgdaBound{x} \AgdaSymbol{→} \AgdaBound{A}
may seem strange, but they are just Agda's way of writing 
\AgdaSymbol{∀} \AgdaBound{x} \AgdaBound{A} and 
\AgdaSymbol{∃} \AgdaBound{x} \AgdaBound{A}.
For any \AgdaBound{V}, \AgdaBound{k}, and \AgdaBound{d}, 
\AgdaSymbol{(}\AgdaBound{V} \AgdaFunction{/} \AgdaBound{k} \AgdaFunction{↦} \AgdaBound{d}\AgdaSymbol{)} 
is an updated variable assignment obtained from \AgdaBound{V} by assigning 
\AgdaBound{d} to the variable $x_k$, and pushing the assignments of all variables 
larger than $x_k$ by one.
E.g., \AgdaSymbol{(}\AgdaBound{V} 
\AgdaFunction{/} \AgdaNumber{0} \AgdaFunction{↦} \AgdaBound{x}\AgdaSymbol{)}
\AgdaNumber{0} \AgdaSymbol{=} \AgdaBound{x} 
and 
\AgdaSymbol{(}\AgdaBound{V} 
\AgdaFunction{/} \AgdaNumber{0} \AgdaFunction{↦} \AgdaBound{x}\AgdaSymbol{)} 
\AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaBound{m}\AgdaSymbol{)} \AgdaSymbol{=} \AgdaBound{V} \AgdaBound{m}.


Now we can define (un)satisfiability of sequents in terms of formula valuations: 
\ExecuteMetaData[sound.tex]{sat}
The \AgdaFunction{respects-eq}\AgdaSpace{}\AgdaBound{R} clause asserts that the
relation assignment \AgdaBound{R} respects equality. This condition is necessary
because we are targeting first-order logic with equality, and we are only 
interested in interpretations that satisfy all equality axioms.

VTV's formalization of first-order semantics is atypical in that (1) every functor
doubles as both relation and function symbols with infinite arities, and 
(2) the definition of satisfiability involves variable assignments, thereby applying to
open as well as closed formulas. But this is completely harmless for our purposes:
whenever a traditional interpretation (with unique arities for each functor and no 
variable assignment) $M$ satisfies a set of sentences $\Gamma$, $M$ can be easily extended 
to an interpretation in the above sense that still satisfies $\Gamma$, since the 
truths of sentences in $\Gamma$ are unaffected by functors or variables that do not 
occur in them. Therefore, if a set of sentences is unsatisfiable in the sense we've
defined above, it is also unsatisfiable in the usual sense. 

Now we finally come to the soundness statement for \AgdaFunction{verify}:
\ExecuteMetaData[sound.tex]{verify-sound}
\AgdaFunction{T} \AgdaSymbol{:} \AgdaFunction{Bool} \AgdaSymbol{→} \AgdaFunction{Set} 
maps Boolean values \AgdaInductiveConstructor{true} and \AgdaInductiveConstructor{false} to
the trivial and empty sets \AgdaFunction{⊤} and \AgdaFunction{⊥}, respectively.
The condition \AgdaFunction{good}\AgdaSpace{}\AgdaBound{S} is necessary, because 
the soundness of TESC proofs is dependent on the invariant that all sequents are good
(in the sense we defined in Section \ref{sec:proof-calc}).
But we can do better than merely assuming that the input sequent is good,
because the parser which converts the input character list into the initial (i.e., root) 
sequent is designed to fail if the parsed sequent is not good. \AgdaFunction{parse-verify}
is the outer function which accepts two character lists as argument, parses them 
into a \AgdaFunction{Sequent} and a \AgdaFunction{Proof}, and calls \AgdaFunction{verify}
on them. The soundness statement for \AgdaFunction{parse-verify} is as follows:
\ExecuteMetaData[sound.tex]{parse-verify-sound}
In other words, if \AgdaFunction{parse-verify} succeeds on two input character lists, 
then the sequent parser successfully parses the first character list into some unsatisfiable 
sequent $S$. \AgdaFunction{parse-verify-sound} is an improvement over \AgdaFunction{verify-sound},
but it also shows the limitation of the current setup. It only asserts that there is 
\textit{some} unsatisfiable sequent parsed from the input characters, 
but provides no guarantees that this sequent is actually equivalent to the original
TPTP problem. This means that the formal verification of VTV is limited to the soundness 
of its proof-checking kernel, and the correctness its TPTP parsing phase has to be 
taken on faith.



\section{Test Results} \label{sec:test-results}

The performance of VTV was tested by running it on all eligible problems 
in the TPTP \cite{sutcliffe2009tptp} problem library. A TPTP problem is eligible if it 
satisfies all of the following conditions (parenthesized numbers indicate 
the total number of problems that satisfy all of the preceding conditions).
\begin{itemize}
  \item It is in the CNF or FOF language (17053). 
  \item Its status is `theorem' or `unsatisfiable' (13636).
  \item It conforms to the official TPTP syntax. More precisely, 
    it does not have any occurrences of the character `\%' in the 
    \verb|sq_char| syntactic class, as required by the TPTP syntax.
    This is important because T3P assumes that the input TPTP problem 
    is syntactically correct and uses `\%' as an endmarker (13389).
  \item All of its formulas have unique names. T3P requires this condition 
    in order to unambiguously identify formulas by their names during 
    proof compilation (13119).
  \item It can be solved by Vampire \cite{riazanov2002design} or E \cite{schulz2002brainiac} 
      in one minute using default settings (Vampire = 7885, E = 4853 \footnote{
      It should be noted that the default setting is actually not 
      optimal for E. When used with the \verb|--auto| or \verb|--auto-schedule|
      options, E's success rates are comparable to that of Vampire. 
      But the TSTP solutions produced by E under these higher-performance modes 
      are much more difficult to compile down to a TESC proof, so they 
      could not be used for these tests.
    }).
  \item The TSTP solution produced by Vampire or E can be compiled to 
    a TESC proof by T3P (Vampire = 7792, E = 4504).
\end{itemize}
The resulting 7792 + 4504 = 12296 proofs were used for testing VTV.
All tests were performed on Amazon EC2 \texttt{r5a.large} instances, 
running Ubuntu Server 20.04 LTS on 2.5 GHz AMD EPYC 7000 processors with 
16 GB RAM. 
\footnote{
  More information on the testing setup and a complete CSV of test results,
  see \url{https://github.com/skbaek/tesc/tree/itp}.
}

Out of the 12296 proofs, there were 5 proofs that VTV failed to verify 
due to exhausting the 16 GB available memory. A cactus plot of verification 
times for the remaining 12291 proofs are shown in Fig. \ref{fig:time}. 
As a reference point, we also show the plot for the default TESC verifier 
included in T3P running on the same proofs. The default TESC verifier 
is written in Rust, and is optimized for performance with no regard to 
verification. For convenience, we refer to it as the Optimized TESC Verifier (OTV).

\input{timechart}

VTV is slower than OTV as expected, but the difference is unlikely to be noticed 
in actual use since the absolute times for most proofs are very short, and the 
total times are dominated by a few outliers. 
VTV verified >97.4\% of proofs under 1 second, and >99.3\% under 5 seconds. 
Also, the median time for VTV is 40 ms, a mere 10 ms behind the OTV's 30 ms. 
But OTV's mean time (54.54 ms) is still much shorter than that of VTV (218.93 ms),
so users may prefer to use it for verifying one of the hard outliers or processing
a large batch of proofs at once.

The main drawback of VTV is its high memory consumption. Fig. \ref{fig:mem} shows
the peak memory usages of the two verifiers. For a large majority of proofs, 
memory usages of both verifiers are stable and stay within the 14-20 MB range,
but VTV's memory usage spikes earlier and higher than OTV. Due the limit of the system 
used, memory usage could only be measured up to 16 GB, but the actual peak for VTV 
would be higher if we included the 5 failed verifications. A separate test running 
VTV on an EC2 instance with 64 GB ram (\texttt{r5a.2xlarge}) still failed for 3 of 
the 5 problematic proofs, so the memory requirement for verifying all 12296 proofs 
with VTV is at least >64 GB. In contrast, OTV could verify all 
12296 proofs with less than 3.2 GB of memory.

\input{memchart}

\section{Conclusion}  \label{sec:conclusion}

The robust test results show that VTV can serve as a fallback option when extra rigour 
is required in verification, thereby increasing our confidence in the correctness of TESC proofs. 
It can also help the design of other TESC verifiers by providing a reference implementation 
that is guaranteed to be correct. On a more general note, VTV is an example of a first-order logic 
formalization that strikes a practical balance between ease of proofs and efficient computation 
while avoiding some common pitfalls like non-termination and complex arity checking. 
Therefore, it can provide a useful starting point for other verified programming projects 
that use first-order logic.

There are two main ways in which VTV could be further improved. Curbing its 
memory usage would be the most important prerequisite for making it the 
default verifier in T3P. This may require porting VTV to a verified programming
language with finer low-level control over memory usage. 

VTV could also benefit from a more reliable TPTP parser. A formally verified 
parser would be ideal, but the complexity of TPTP's syntax makes it difficult  
to even \textit{specify} the correctness of a parser, let alone prove it. 
A more realistic approach would be imitating the technique used by verified 
LRAT checkers \cite{heule2017efficient}, making VTV print the parsed problem 
and textually comparing its output with the original problem.



\section*{Acknowledgements}  \label{sec:ack}

This work has been partially supported by AFOSR grant FA9550-18-1-0120.

Thanks to Jeremy Avigad for a careful proofreading of drafts and advice for improvements.



\bibliographystyle{plain}

\bibliography{references}



\appendix  

\section{Formula analysis functions} \label{apdx:faf}

\input{faf.tex}

\end{document}

















