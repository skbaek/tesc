\documentclass[12pt]{article}
\usepackage{bussproofs, array, amstext, amssymb} 
\usepackage{mathtools, extarrows}
\usepackage{latexsym}
\usepackage{syntax}
\usepackage{tabularx}
\usepackage{flowchart}
\usepackage{array,multirow}
\usepackage[section]{placeins}
\usepackage[bottom]{footmisc}
\usepackage{tablefootnote}
\usepackage[nodayofweek,level]{datetime}
\usepackage[affil-it]{authblk}
\usepackage{hyperref}
\usepackage{algorithm,algpseudocode}
\usepackage{textcomp}
\usepackage{pgfplots}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{catchfilebetweentags}
\usepackage{agda}
\usepackage[verbose]{newunicodechar}

\input{unicode}

\pgfplotsset{compat=1.14}
\definecolor{VTV}{HTML}{019529}
\definecolor{OTV}{HTML}{0C06F7}

\algrenewcommand{\algorithmiccomment}[1]{\hskip3em$\rhd$ #1}
\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algdef{SE}[SWITCH]{Case}{EndCase}[1]{\textbf{case}\ #1\ \textbf{of}}{\algorithmicend\ \algorithmicswitch}%
\algtext*{Indent}
\algtext*{EndIndent}
\algtext*{EndCase}
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFunction}% Remove "end if" text

\renewcommand{\labelitemii}{$\star$}
\newcommand{\midtilde}{\raisebox{0.5ex}{\texttildelow}}
\newcommand{\at}[0]{\ @\ }
\newcommand{\An}[0]{\mathrm{An}}
\newcommand{\len}[0]{\mathrm{len}}
\newcommand{\Gnd}[0]{\mathit{gnd}}
\newcommand{\Prf}[0]{\mathit{prf}}
\newcommand{\Bch}[0]{\mathit{bch}}
\newcommand{\Bchs}[0]{\mathit{bchs}}
\newcommand{\Prob}[0]{\mathit{prob}}
\newcommand{\idf}[1]{\#_{#1}} 
\newcommand{\fresh}[1]{\mathit{fresh}_{\#}({#1})} 
\newcommand{\lfi}[0]{\mathrm{lfi}} 
\newcommand{\Dec}[1]{\xRightarrow{#1}} 
\newcommand{\bprf}[0]{\begin{prooftree}}
\newcommand{\eprf}[0]{\end{prooftree}}
\newcommand{\axm}[1]{\AxiomC{$#1$}}
\newcommand{\unr}[1]{\UnaryInfC{$#1$}}
\newcommand{\bnr}[1]{\BinaryInfC{$#1$}}
\newcommand{\tnr}[1]{\TrinaryInfC{$#1$}}
\newcommand{\rlb}[1]{\RightLabel{#1}}
\newcommand{\limp}[0]{\to}
\newcommand{\liff}[0]{\leftrightarrow}
\newcommand{\sq}[1]{\text{`#1'}} 

\newcommand{\size}[0]{\mathrm{size}}
\newcommand{\adm}[0]{\mathrm{adm}}

 
\newcommand{\concat}{%
  \mathbin{{+}\mspace{-8mu}{+}}%
}

\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \begin{center}%
  \let \footnote \thanks
    {\Large\bfseries \@title \par}%
    \vskip 1.5em%
    {\normalsize
      \lineskip .5em%
      \begin{tabular}[t]{c}%
        \@author
      \end{tabular}\par}%
    \vskip 1em%
    {\normalsize \@date}%
  \end{center}%
  \par
  \vskip 1.5em}
\makeatother

\title{A Formally Verified TESC Verifier}

\author{Seulkee Baek}

\begin{document}

\maketitle

\begin{abstract}

The Verified TESC Verifier (VTV) is a formally verified checker for the new 
Theory-Extensible Sequent Calculus (TESC) proof format for first-order ATPs. 
VTV accepts a TPTP problem and a TESC proof as input, and uses the latter to 
verify the unsatisfiability of the former. VTV is written in Agda, and the 
soundness of its proof-checking kernel is verified in respect to a first-order
semantics formalized in Agda. VTV shows robust performance in a comprehensive 
test using all eligible problems from the TPTP problem library, successfully 
verifying all but the largest 5 of 12296 proofs, with >97\% of the 
proofs verified under 1 second. 

\end{abstract}

\section{Introduction}

Modern automated reasoning tools are highly complex software whose correctness 
is no simple matter to establish. Bugs have been discovered in them in the past 
\cite{reger2017testing,harrison2006towards}, and more are presumably hidden in 
systems used today. One popular strategy for coping with the possibility of errors 
in automated reasoning is the \textit{De Bruijn} criterion \cite{barendregt2005challenge}, 
which states that automated reasoning software should produce `proof objects' which can be 
independently verified by simple checkers that users can easily understand 
and trust. In addition to reducing the trust base of theorem provers, 
the De Bruijn criterion comes with the additional benefit that the 
small trusted core is often simple enough to be formally verified themselves. 
Such thoroughgoing verification is far from universal, but there has been notable 
progress toward this goal in various subfields of automated reasoning, including 
interactive theorem provers, SAT solvers, and SMT solvers.

One area in which similar developments have been conspicuously absent is 
first-order automated theorem provers (ATPs), where the lack of a machine-checkable
proof format \cite{reger2017checkable} precluded any simple independent verifiers, 
let alone a formally verified one. The Theory-Extensible Sequent Calculus (TESC) is a new 
proof format for first-order ATPs designed to fill this gap. In particular,
the format's small set of fined-grained inference rules makes it relatively 
easy to implement and verify its proof checker.

This paper presents the Verified TESC Verifier (VTV), a proof checker for  
the TESC format written and verified in Agda \cite{bove2009brief}. 
The aim of the paper is twofold. Its primary purpose is to demonstrate the 
reliability of TESC proofs by showing precisely what is established by their successful 
verification using VTV. Its secondary aim is to serve as a guide for understanding 
VTV's codebase and its design choices, which can be especially useful for readers 
who want to implement their own TESC verifiers.

The rest of the paper is organized as follows:
Section \ref{sec:rel-works} gives a brief survey of similar works and how VTV relates to them.
Section \ref{set:proof-calc} describes the syntax and inference rules of the TESC proof calculus.
Section \ref{sec:verifier} presents the main TESC verifier kernel, and
Section \ref{sec:soundness} gives a detailed specification of the verifier's soundness property.
Sections \ref{set:proof-calc}, \ref{sec:verifier}, and \ref{sec:soundness} also include code 
excerpts and discuss how their respective contents are formalized in Agda.
Section \ref{sec:test-results} shows the  results of empirical tests measuring VTV's performance.
Section \ref{sec:conclusion} gives a summary and touches on potential future work.

\section{Related Works} \label{sec:rel-works}

SAT solving is arguably the most developed subfield of automated reasoning in terms of 
verified proof checkers. A non-exhaustive list of SAT proof formats with verified checkers
include RAT \cite{heule2013verifying}, RUP and IORUP \cite{heule2014bridging},
LRAT \cite{cruz2017lrat}, and GRIT \cite{cruz2017grit}. In the related field of SMT solving,
the SMTCoq project \cite{armand2011modular} also uses a proof checker implemented and
verified in the Coq proof assistant.

Despite the limitations imposed by G\"odel's second incompleteness theorem \cite{godel1931formal},
there has been interesting work toward self-verification of interactive 
theorem prover kernels. All of HOL Light except the axiom of infinity has been 
proven consistent in HOL Light itself \cite{harrison2006towards}, which allows us to consider HOL Light
consistent for most proofs in practice. More recent work on the Metamath Zero \cite{carneiro2019metamath}
theorem prover aims to not only verify a large part of Metamath Zero's logic in itself, 
but also its implementation down to the level of x84-64 instruction set architecture. 

VTV is designed to serve a role similar to these verified checkers for first-order ATPs
and the TESC format.
There has been several different approaches to verifying the output of ATPs, but 
(to the extent of my knowledge) none has used a proof format with a verified checker. 
For instance, GDV \cite{sutcliffe2006semantic} works by breaking down an ATP-generated 
solution into small subproblems and re-solving them with multiple unverified ATPs. 
Foundational Proof Certificates \cite{chihani2013foundational} is a system that could 
be used to specify proof formats and implement proof checkers for first-order ATPs.
Its actual implementation, however, \cite{chihani2015proof} has been limited to a small 
subset of inferences used by ATPs and an unverified checker.



\section{Proof Calculus} \label{set:proof-calc} 

The syntax of the TESC calculus is as follows:
\begin{align*}
f ::= &\ \sigma\ |\ \idf{k}\\
t ::= &\ x_k\ |\ f(\vec{t})\\
\vec{t} ::= &\ \cdot\ |\ \vec{t}, t\\
\phi ::= &\ \top\ |\ \bot\ |\ f(\vec{t})\ |\ \lnot \phi\ |\ \phi \lor \chi\ |\ \phi \land \chi\ |\ \phi \to \chi\ |\ \phi \leftrightarrow \chi\ |\ \forall \phi\ |\ \exists \phi\\
\Gamma ::= &\ \cdot\ |\ \Gamma, \phi
\end{align*}
$f$ ranges over \textit{functors}, which usually called `non-logical symbols' in
other presentations of first-order logic. The TESC calculus makes no distinction
between function and relation symbols, and relies on the context to determine whether 
a symbol applied to arguments is a term or an atomic formula. For brevity, we borrow
the umbrella term `functor' from the TPTP syntax and use it to refer to any non-logical symbol.
There are two types of functors: $\sigma$ ranges over \textit{plain functors}, 
which you can think of as the usual relation or function symbols. We assume that there 
is a suitable set of symbols $\Sigma$, and let $\sigma \in \Sigma$.

Symbols of the form $\idf{k}$ are \textit{indexed functors}, and the number $k$ is 
called the \textit{functor index} of $\idf{k}$. Indexed functors are used to reduce 
the cost of introducing fresh functors: if you keep track of the largest functor index
$k$ that occurs in the environment, you may safely use $\idf{k+1}$ as a fresh functor
without costly searches over a large number of terms and formulas.

$t$ ranges over terms, $\vec{t}$ over lists of terms, $\phi$ over formulas, and
$\Gamma$ over sequents.
Quantified formulas are written without variables thanks to the use of De Bruijn 
indices \cite{de1972lambda}; the number $k$ in variable $x_k$ is its De Bruijn index. 

Formalization of TESC syntax in Agda is mostly straightforward, but with one 
caveat: the first-instinct definition of terms as 
\ExecuteMetaData[wrongterm.tex]{wrongterm}
works poorly in practice, since any structural recursion on terms immediately 
runs into non-termination issues. Although it is possible to manually prove 
termination, it is much simpler to sidestep this issue with a pseudo-mutual recursion:
\ExecuteMetaData[basic.tex]{termstar}
which lets us define terms and lists of terms as
\ExecuteMetaData[basic.tex]{termterms}

\input{inf-rules.tex}
\input{faf.tex}

The inference rules of the TESC calculus are shown in Table \ref{tab:inf-rules}.
The $A$,$B$,$C$,$D$, and $N$ rules are the \textit{analytic} rules which analyze  
existing formulas on the sequent and adds resulting subformulas to the new 
sequent (we are reading the proof tree in the bottom-up direction).
The formula analysis functions used by rules are given in Table 
\ref{tab:faf}. Notice that the analytic rules are very similar to 
Smullyan's \textit{uniform notation} for analytic tableaux, which is where 
they get their names from. Note that: 
\begin{itemize}
  \item $\Gamma[i]$ denotes the (0-based) $i$th formula of sequent $\Gamma$, 
    where $\Gamma[i] = \top$ if the index $i$ is out-of-bounds. 
  \item $\lfi(x)$ denotes the largest functor index (lfi) occurring in $x$. 
    If $x$ incudes no functor indices, $\lfi(x) = -1$.
  \item $\size(\Gamma)$ is the number of formulas in $\Gamma$.
  \item All inference rules are designed to preserve the invariant
    $\lfi(\Gamma) < \size(\Gamma)$ for every sequent $\Gamma$. We say that 
    a sequent $\Gamma$ is \textit{good} if it satisfies this invariant.
  \item $S$ is the usual cut rule, and $X$ is the axiom or init rule. 
  \item $\adm(k,\phi)$ asserts that $\phi$ is an \textit{admissable} formula in 
    respect to the target theory and sequent size $k$. More precisely, if $T$ 
    is the target theory, $\Gamma$ is a sequent satisfiable modulo $T$, and 
    $\size(\Gamma) = k$, then $\adm(k,\phi)$ implies that adding $\phi$ to 
    $\Gamma$ preserves satisfiability modulo $T$. The sequent size argument $k$ 
    is required because some admissable formulas use this number as the functor 
    index of newly introduced indexed functors. The $T$ rule may be used to add 
    any admissable formula.
\end{itemize}
The last part implies that the definition of the admissability predicate, and by 
extension the definition of well-formed TESC proofs, changes according to the implicit 
target theory. This is the `theory-extensible' part of TESC. The current version of 
VTV verifies basic TESC proofs that target the theory of equality, so it allows $T$ 
rules to introduce equality axioms, fresh relation symbol definitions, and choice axioms.
But VTV can be easily modified in a modular way to target other theories as well.

TESC proofs are formalized in Agda as follows:
\ExecuteMetaData[verify.tex]{proof}
There are parts of TESC proofs omitted in the definition of 
\AgdaFunction{Proof}, e.g. sequents. This is a design choice made in favor of
efficient space usage. Since proofs are uniquely determined by their
root sequents + complete information of the inference rules used,
TESC proof files save space by omitting any components that can be 
constructed on the fly during verification, which includes all intermediate 
sequents and formulas introduced by analytic rules. Terms of the type 
\AgdaFunction{Proof} are constructed by parsing input TESC files, 
so it only includes information stored in TESC files, which are the arguments 
to the constructors of \AgdaFunction{Proof}.



\section{The Verifier} \label{sec:verifier} 

Since \AgdaFunction{Proof} only includes basic information regarding inference 
rule applications, the verifier function for \AgdaFunction{Proof} must construct 
intermediate sequents as it recurses down a proof, and also check that inference 
rule arguments (e.g., the term $t$ of a $C$ rule application) satisfy their side 
conditions. This is exactly what \AgdaFunction{verify} does:
\ExecuteMetaData[verify.tex]{verify}
Analytic rules introduce new formulas obtained by formula analysis using 
\AgdaFunction{analyze} functions, and side conditions are checked using appropriate 
\AgdaFunction{?} functions. The argument type \AgdaFunction{Sequent}, however, 
offers some interesting design choices. What kind of data structures should be used 
to encode sequents? The first version of VTV used lists, but lists immediately become 
a bottleneck with practically-sized problems due to their poor random access speeds. 
The default TESC verifier included in the TPTP-TSTP-TESC Processor (T3P) tool uses arrays, 
but arrays are hard to come by and even more difficult to reason about in dependently 
typed languages like Agda. Self-balancing trees like AVL or red-black trees come 
somewhere between lists and arrays in terms of convenience and performance, but it 
can still be tedious to prove basic facts about them if those proofs are not 
available in your language of choice, as is the case for Agda's standard library.

For VTV, we cut corners by taking advantage of the fact that (1) formulas 
are never deleted from sequents, and (2) new formulas are always added to the 
end of sequents. This allows us to use a simple tree structure:
\ExecuteMetaData[basic.tex]{tree}
For any tree \AgdaInductiveConstructor{fork} \AgdaBound{k} \AgdaBound{a} \AgdaBound{t} \AgdaBound{s},
the number \AgdaBound{k} is the size of the left subtree \AgdaBound{t}. 
This property is not guaranteed to hold by construction,
but it is easy to ensure that it always holds in practice. With this definition,
balanced addition of elements to trees becomes trivial:
\ExecuteMetaData[basic.tex]{add}
Then the type \AgdaFunction{Sequent} can be defined as \AgdaFunction{Tree Formula}.



\section{Soundness} \label{sec:soundness}

In order to verify the soundness of \AgdaFunction{verify}, we first need to formalize 
a first-order semantics that it can be sound in respect to. Most of the formalization 
is routine, but it also includes some oddities particular to VTV.

One awkward issue that recurs in formalization of first-order semantics is the 
handling of arities. Given that each functor has a unique arity,
what do you do with ill-formed terms and atomic formulas with the wrong 
number of arguments? You must either tweak the syntax definition to 
preclude such possibilities, or deal with ill-formed terms and formulas as 
edge cases, both of which can lead to unpleasant bloat. 

For VTV, we avoid this issue by assuming that every functor has infinite arities.
Or rather, for each functor $f$ with arity $k$, there are an infinite number 
of other functors that share the name $f$ and has arities $0, 1, ..., k-1, k+1, k+2, ...$ 
ad infinitum. With this assumption, the denotation of functors can be 
simply defined as 
\ExecuteMetaData[sound.tex]{relfun}
A \AgdaFunction{Rels} (resp. \AgdaFunction{Funs}) can be thought of as a 
collection of an infinite number of relations (resp. functions), one for 
each arity. An interpretation is a pair of a relation assignment and a 
function assignment, which assign \AgdaFunction{Rels} and \AgdaFunction{Funs} 
to functors.
\ExecuteMetaData[sound.tex]{rafa}
variable assignments simply assign denotations to \AgdaFunction{Nat}, since variables are 
identified by their Bruijn indices.
\ExecuteMetaData[sound.tex]{va}

Term valuation requires a bit of tweaking due to the unusual definition of \AgdaFunction{Term*}:
\ExecuteMetaData[sound.tex]{termval}
Formula valuation is mostly straightforward, but some care needs to be taken 
in the handling of variable assignments. The truth value of universally quantified 
formulas are defined as:
\ExecuteMetaData[sound.tex]{univ-val}
\AgdaInductiveConstructor{qtf} \AgdaSymbol{:} \AgdaFunction{Bool} \AgdaSymbol{→} \AgdaFunction{Formula} \AgdaSymbol{→} \AgdaFunction{Formula} 
is a constructor of \AgdaFunction{Formula} for quantified formulas, and \AgdaInductiveConstructor{qtf} \AgdaInductiveConstructor{false}
encodes the universal quantifier.
For any \AgdaBound{V}, \AgdaNumber{k}, and \AgdaBound{d}, 
\AgdaSymbol{(}\AgdaBound{V} \AgdaFunction{/} \AgdaNumber{k} \AgdaFunction{↦} \AgdaBound{d}\AgdaSymbol{)} 
is an updated variable assignment obtained from \AgdaBound{V} by assigning 
\AgdaBound{d} to the variable $x_k$, and pushing the assignments of all variables 
larger than $x_k$ by one.
E.g., \AgdaSymbol{(}\AgdaBound{V} 
\AgdaFunction{/} \AgdaNumber{0} \AgdaFunction{↦} \AgdaBound{x}\AgdaSymbol{)}
\AgdaNumber{0} \AgdaSymbol{=} \AgdaBound{x} 
and 
\AgdaSymbol{(}\AgdaBound{V} 
\AgdaFunction{/} \AgdaNumber{0} \AgdaFunction{↦} \AgdaBound{x}\AgdaSymbol{)} 
\AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaNumber{m}\AgdaSymbol{)} \AgdaSymbol{=} \AgdaBound{V} \AgdaNumber{m}.


Now we can define (un)satisfiability of sequents in terms of formula valuations: 
\ExecuteMetaData[sound.tex]{sat}
The \AgdaFunction{respects-eq}\AgdaSpace{}\AgdaBound{R} clause asserts that the
relation assignment \AgdaBound{R} respects equality. This condition is necessary
because we are targeting first-order logic with equality, and we are only 
interested in interpretations that satisfy all equality axioms.

VTV's formalization of first-order semantics is atypical in that (1) every functor
doubles as both relation and function symbols with infinite arities, and 
(2) the definition of satisfiability involves variable assignments, thereby applying to
open as well as closed formulas. But this is completely harmless for our purposes:
whenever a traditional interpretation (with unique arities for each functor and no 
variable assignment) $M$ satisfies a set of sentences $\Gamma$, $M$ can be easily extended 
to an interpretation in the above sense that still satisfies $\Gamma$, since the 
truths of sentences in $\Gamma$ are unaffected by functors or variables that do not 
occur in them. Therefore, if a set of sentences is unsatisfiable in the sense we've
defined above, it is also unsatisfiable in the usual sense. 

Now we finally come to the soundness statement for \AgdaFunction{verify}:
\ExecuteMetaData[sound.tex]{verify-sound}
\AgdaFunction{T} \AgdaSymbol{:} \AgdaFunction{Bool} \AgdaSymbol{→} \AgdaFunction{Set} 
maps \AgdaInductiveConstructor{true} to \AgdaFunction{⊤} and \AgdaInductiveConstructor{false} to \AgdaFunction{⊥}.
The condition \AgdaFunction{good}\AgdaSpace{}\AgdaBound{S} is necessary, because 
the soundness of TESC proofs is dependent on the invariant that all sequents are good. 
But we can do better than merely assuming that the input sequent is good,
because the parser which converts the input character list into the initial (i.e., root) 
sequent is designed to fail if the parsed sequent is not good. \AgdaFunction{parse-verify}
is the outer function which accepts two character lists as argument, parses them 
into a \AgdaFunction{Sequent} and a \AgdaFunction{Proof}, and calls \AgdaFunction{verify}
on them. The soundness statement for \AgdaFunction{parse-verify} is as follows:
\ExecuteMetaData[sound.tex]{parse-verify-sound}
In other words, if \AgdaFunction{parse-verify} succeeds on two input character lists, 
then the sequent parser successfully parses the first character list into some unsatisfiable 
sequent $S$. \AgdaFunction{parse-verify-sound} is an improvement over \AgdaFunction{verify-sound},
but it also shows the limitation of the current setup. It only asserts that there is 
\textit{some} unsatisfiable sequent parsed from the input characters, 
but provides no guarantees that the parsed sequent is actually equivalent to the original
TPTP problem. This means that the formal verification of VTV is limited to the soundness 
of its proof-checking kernel, and the correctness its TPTP parsing phase has to be 
taken on faith.



\section{Test Results} \label{sec:test-results}

The performance of VTV was tested by running it on all eligible problems 
in the TPTP \cite{sutcliffe2009tptp} problem library. A TPTP problem is eligible if it 
satisfies all of the following conditions (parenthesized numbers indicate 
the total number of problems that satisfy all of the preceding conditions).
\begin{itemize}
  \item It is in the CNF or FOF language (23291). 
  \item Its status is `theorem' or `unsatisfiable' (13636).
  \item It conforms to the official TPTP syntax. More precisely, 
    it does not have any occurrences of the character `\%' in the 
    \verb|sq_char| syntactic class, as required by the TPTP syntax.
    This is important because T3P assumes that the input TPTP problem 
    is syntactically correct and uses `\%' as an endmarker (13389).
  \item All of its formulas have unique names. T3P requires this condition 
    in order to unambiguously identify formulas by their names during 
    proof compilation (13119).
  \item It can be solved by Vampire or E in one minute using 
    default settings (Vampire = 7885, E = 4853).
  \item The TSTP solution produced by Vampire or E can be compiled to 
    a TESC proof by T3P (Vampire = 7792, E = 4504).
\end{itemize}
The resulting 7792 + 4504 = 12296 proofs were used for testing VTV.
All tests were performed on Amazon EC2 \texttt{r5a.large} instances, 
running Ubuntu Server 20.04 LTS on 2.5 GHz AMD EPYC 7000 processors with 
16 GB RAM. 
\footnote{
  More information on the testing setup used, including the versions of 
  software used, can be found at \url{https://github.com/skbaek/tesc/tree/itp}.
}

Out of the 12296 proofs, there were 5 proofs that VTV failed to verify 
due to exhausting the 16 GB available memory. A cactus plot of verification 
times for the remaining 12291 proofs are shown in Fig. \ref{fig:time}. 
As a reference point, we also show the plot for the default TESC verifier 
included T3P running on the same proofs. The default TESC verifier 
is written in Rust, and is optimized for performance with no regard to 
verification. For convenience, we refer to it as the Optimized TESC Verifier (OTV).

\input{timechart}

VTV is slower than OTV as expected, but the difference is unlikely to be noticed 
in actual use since the absolute times for most proofs are very short, and the 
total times are dominated by a few outliers. 
VTV verified >97.4\% of proofs under 1 second, and >99.3\% under 5 seconds. 
Also, the median time for VTV is 40 ms, a mere 10 ms behind the OTV's 30 ms. 
But OTV's mean time (54.54 ms) is still much shorter than that of VTV (218.93 ms),
so users may prefer to use it for verifying one of the hard outliers or processing
a large batch of proofs at once.

The main drawback of VTV is its high memory consumption. Fig. \ref{fig:mem} shows
the peak memory usages of the two verifiers. For a large majority of proofs, 
memory usages of both verifiers are stable and stay within the 14-20 MB range,
but VTV's memory usage spikes earlier and higher than OTV. Due the limit of the system 
used, memory usage could only be measured up to 16 GB, but the actual peak for VTV 
would be higher if we included the 5 failed verifications. A separate test running 
VTV on an EC2 instance with 64 GB ram (\texttt{r5a.2xlarge}) still failed for 3 of 
the 5 problematic proofs, so the memory requirement for verifying all 12296 proofs 
with VTV is at least >64 GB. In contrast, OTV could verify all 
12296 proofs with less than 3.2 GB of memory.

\input{memchart}

\section{Conclusion}  \label{sec:conclusion}

VTV can serve as a fallback option whenever extra rigour is required in verification, 
thereby increasing our confidence in the correctness of TESC proofs. 
It also helps the design of other TESC verifiers by providing a reference 
implementation that is guaranteed to be correct. It should be particularly helpful 
for implementing other verified TESC verifiers in, say, Lean or Coq, since many 
of the issues we've discussed (termination checking, proving properties of trees, etc.) 
are common to these languages. 

There are two main ways in which VTV could be further improved. Curbing its 
memory usage would be the most important prerequisite for making it the 
default verifier in T3P. This may require porting VTV to a verified programming
language with finer low-level control over memory usage. 

VTV could also benefit from a more reliable TPTP parser. A formally verified 
parser would be ideal, but the complexity of TPTP's syntax makes it difficult  
to even \textit{specify} the correctness of a parser, let alone prove it. 
A more realistic approach would be imitating the technique used by verified 
LRAT checkers \cite{heule2017efficient}, making VTV print the parsed problem 
and textually comparing its with the original problem.

\bibliographystyle{plain}

\bibliography{references}

\end{document}

















